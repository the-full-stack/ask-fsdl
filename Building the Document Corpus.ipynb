{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1de5220b",
   "metadata": {},
   "source": [
    "<img src=\"https://fsdl.me/logo-720-dark-horizontal\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae580e2f",
   "metadata": {},
   "source": [
    "# Building the Document Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd1ee6dd",
   "metadata": {},
   "source": [
    "This notebook constructs a corpus of documents\n",
    "from the\n",
    "[Full Stack Deep Learning](https://fullstackdeeplearning.com/course/2022)\n",
    "course,\n",
    "the\n",
    "[Full Stack Large Language Models Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp),\n",
    "and a\n",
    "[collection of papers about LLMs](https://tfs.ai/llm-lit-review)\n",
    "and sends it to a [managed MongoDB database](https://www.mongodb.com/docs/atlas/).\n",
    "\n",
    "These documents are then used to support LLM-powered Q&A."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8220937f",
   "metadata": {},
   "source": [
    "To achieve higher quality results,\n",
    "we use specialized parsing for the sources.\n",
    "\n",
    "Data preparation is less exciting, but often higher yield, than modeling or engineering!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "470507c2",
   "metadata": {},
   "source": [
    "## Target Format"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a0ef85f",
   "metadata": {},
   "source": [
    "For sourced Q&A, we want to store a collection of documents.\n",
    "\n",
    "In this context, a document is text plus some optional metadata --\n",
    "including, ideally, a URL source and an identifier.\n",
    "\n",
    "A pseudo-schema might look like this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b69c966",
   "metadata": {},
   "source": [
    "```json\n",
    "Docs = {[Document]}\n",
    "\n",
    "Document = {\n",
    "    text: string,\n",
    "    metadata: Metadata?\n",
    "}\n",
    "    \n",
    "Metadata = {\n",
    "    source: url?\n",
    "    sha256: hash\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7d1058d",
   "metadata": {},
   "source": [
    "The Q&A will be more useful the more precisely we slice and link the documents,\n",
    "so we want to split a semantic \"document\", like a lecture or a video,\n",
    "up into sub-documents first."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33186570",
   "metadata": {},
   "source": [
    "**Note**: we leave it up to the `langchain.TextSplitter` to split sub-documents into chunks smaller than a source at time of upsert into the vector database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e512d62",
   "metadata": {},
   "source": [
    "## Markdown Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4aa2f2e7",
   "metadata": {},
   "source": [
    "Most pages on the Full Stack website\n",
    "are originally written in Markdown,\n",
    "which makes it easy to pull out relevant sub-documents."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "736793f8",
   "metadata": {},
   "source": [
    "### Lectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d33fe14",
   "metadata": {},
   "source": [
    "We first define a `DataFrame` with basic metadata about where the lectures can be found -- on the website and as raw Markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64cab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_md_url_base = \"https://raw.githubusercontent.com/full-stack-deep-learning/website/main/docs/course/2022\"\n",
    "website_url_base = \"https://fullstackdeeplearning.com/course/2022\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_slugs = {\n",
    "    1: \"lecture-1-course-vision-and-when-to-use-ml\",\n",
    "    2: \"lecture-2-development-infrastructure-and-tooling\",\n",
    "    3: \"lecture-3-troubleshooting-and-testing\",\n",
    "    4: \"lecture-4-data-management\",\n",
    "    5: \"lecture-5-deployment\",\n",
    "    6: \"lecture-6-continual-learning\",\n",
    "    7: \"lecture-7-foundation-models\",\n",
    "    8: \"lecture-8-teams-and-pm\",\n",
    "    9: \"lecture-9-ethics\"\n",
    "}\n",
    "\n",
    "lecture_df = pd.DataFrame.from_dict(lecture_slugs, orient=\"index\", columns=[\"url-slug\"])\n",
    "lecture_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0664c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_df[\"raw-md-url\"] = lecture_df[\"url-slug\"].apply(lambda s: f\"{lecture_md_url_base}/{s}/index.md\".format(s))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12f981b5",
   "metadata": {},
   "source": [
    "We then bring in the markdown files from GitHub,\n",
    "parse them to split out headings as our \"sources\",\n",
    "and use `slugify` to create URLs for those heading sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19becb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open\n",
    "\n",
    "\n",
    "def get_text_from(url):\n",
    "    with open(url) as f:\n",
    "        contents = f.read()\n",
    "    return contents\n",
    "\n",
    "lecture_df[\"raw-text\"] = lecture_df[\"raw-md-url\"].apply(lambda url: get_text_from(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_headings_and_slugs(text):\n",
    "    import mistune\n",
    "    from slugify import slugify\n",
    "\n",
    "    markdown_parser = mistune.create_markdown(renderer=\"ast\")\n",
    "    parsed_text = markdown_parser(text)\n",
    "    \n",
    "    heading_objects = [obj for obj in parsed_text if obj[\"type\"] == \"heading\"]\n",
    "    h2_objects = [obj for obj in heading_objects if obj[\"level\"] == 2]\n",
    "    \n",
    "    targets = [obj for obj in h2_objects if not(obj[\"children\"][0][\"text\"].startswith(\"description: \"))]\n",
    "    target_headings = [tgt[\"children\"][0][\"text\"] for tgt in targets]\n",
    "    \n",
    "    heading_slugs = [slugify(target_heading) for target_heading in target_headings]\n",
    "    \n",
    "    return target_headings, heading_slugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987faeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lecture(row):\n",
    "    text = row[\"raw-text\"]\n",
    "    \n",
    "    headings, slugs = get_target_headings_and_slugs(text)\n",
    "    \n",
    "    texts = split_by_headings(text, headings)\n",
    "    slugs = [\"\"] + slugs\n",
    "    \n",
    "    text_rows = []\n",
    "    for text, slug in zip(texts, slugs):\n",
    "        text_rows.append({\n",
    "            \"url-slug\": row[\"url-slug\"] + \"#\" + slug,\n",
    "            \"raw-md-url\": row[\"raw-md-url\"],\n",
    "            \"text\": text,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame.from_records(text_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0633af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_headings(text, headings):\n",
    "    texts = []\n",
    "    for heading in reversed(headings):\n",
    "        text, section = text.split(\"# \" + heading)\n",
    "        texts.append(f\"## {heading}{section}\")\n",
    "    texts.append(text)\n",
    "    texts = list(reversed(texts))\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "lecture_dfs = []\n",
    "for idx, row in lecture_df.iterrows():\n",
    "    single_lecture_df = split_lecture(row)\n",
    "    single_lecture_df[\"lecture-idx\"] = idx\n",
    "    lecture_dfs.append(single_lecture_df)\n",
    "    \n",
    "split_lecture_df = pd.concat(lecture_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lecture_df[\"source\"] = split_lecture_df[\"url-slug\"].apply(lambda s: f\"{website_url_base}/{s}\".format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447af422",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_lecture_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39955f69",
   "metadata": {},
   "source": [
    "## YouTube Videos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9009409",
   "metadata": {},
   "source": [
    "Videos are not text, but transcripts are --\n",
    "so we can also build a corpus based on videos\n",
    "from the FSDL YouTube channel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bafd220",
   "metadata": {},
   "source": [
    "We first define the video metadata\n",
    "and use it to build a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b986c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = {\n",
    "    \"id\": [\"-Iob-FW5jVM\",\n",
    "           \"hltjXcaxExY\",\n",
    "           \"9w8CVuHUk8U\",\n",
    "           \"6fSd8RdtDBs\",\n",
    "           \"lsWLgQyaeik\",\n",
    "           \"BPYOsDCZbno\",\n",
    "           \"NEGDJuINE9E\",\n",
    "           \"RLemHNAO5Lw\",\n",
    "           \"D65SlCSoS-0\",\n",
    "           \"Jlm4oqW41vY\",\n",
    "           \"zoS5Fx2Ou1Y\",\n",
    "           \"W3hKjXg7fXM\",\n",
    "           \"2j6rG-4zS6w\",\n",
    "           \"nra0Tt3a-Oc\",\n",
    "           \"-mKzxSC0r7w\",\n",
    "           \"Rm11UeGwGgk\",\n",
    "           \"a54xH6nT4Sw\",\n",
    "           \"7FQpbYTqjAA\",\n",
    "           \"twHxmU9OxDU\",\n",
    "           \"MyFrMFab6bo\",\n",
    "           \"JnBHR_yL2w8\",\n",
    "           \"YdeuQhlHmCA\",\n",
    "           \"pUKs4xM1r5U\",\n",
    "           \"l5mG4z343qg\",\n",
    "           \"Fquj2u7ay40\",\n",
    "           \"ax_R4yz1WwM\",\n",
    "           ],\n",
    "    \"title\": [\n",
    "        \"Lecture 01: When to Use ML and Course Vision (FSDL 2022)\",\n",
    "        \"Lab Intro and Overview\",\n",
    "        \"Lab 01: Neural networks in PyTorch\",\n",
    "        \"Lab 02: PyTorch Lightning and Convolutional NNs\",\n",
    "        \"Lab 03: Transformers and Paragraphs (FSDL 2022)\",\n",
    "        \"Lecture 02: Development Infrastructure & Tooling (FSDL 2022)\",\n",
    "        \"Lab 04: Experiment Management (FSDL 2022)\",\n",
    "        \"Lecture 03: Troubleshooting & Testing (FSDL 2022)\",\n",
    "        \"Lab 05: Troubleshooting & Testing (FSDL 2022)\",\n",
    "        \"Lecture 04: Data Management (FSDL 2022)\",\n",
    "        \"Lab 06: Data Annotation (FSDL 2022)\",\n",
    "        \"Lecture 05: Deployment (FSDL 2022)\",\n",
    "        \"Lab 07: Web Deployment (FSDL 2022)\",\n",
    "        \"Lecture 06: Continual Learning (FSDL 2022)\",\n",
    "        \"Lab 08: Monitoring (FSDL 2022)\",\n",
    "        \"Lecture 07: Foundation Models (FSDL 2022)\",\n",
    "        \"Lecture 08: ML Teams and Project Management (FSDL 2022)\",\n",
    "        \"Lecture 09: Ethics (FSDL 2022)\",\n",
    "        \"Launch an LLM App in One Hour (LLM Bootcamp)\",\n",
    "        \"LLM Foundations (LLM Bootcamp)\",\n",
    "        \"Learn to Spell: Prompt Engineering (LLM Bootcamp)\",\n",
    "        \"Augmented Language Models (LLM Bootcamp)\",\n",
    "        \"Project Walkthrough: askFSDL (LLM Bootcamp)\",\n",
    "        \"UX for Language User Interfaces (LLM Bootcamp)\",\n",
    "        \"LLMOps (LLM Bootcamp)\",\n",
    "        \"What's Next? (LLM Bootcamp)\",\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baby's first expectation test\n",
    "assert len(videos[\"title\"]) == len(videos[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd24e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.DataFrame.from_dict(videos)\n",
    "videos_df.index = videos_df[\"id\"]\n",
    "videos_df = videos_df.drop(\"id\", axis=\"columns\")\n",
    "videos_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74cbf3ac",
   "metadata": {},
   "source": [
    "We use the `youtube_transcript_api` package\n",
    "to pull down the transcripts\n",
    "in a single line of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d570fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "\n",
    "transcripts = [YouTubeTranscriptApi.get_transcript(video_id) for video_id in videos_df.index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc0e1cf8",
   "metadata": {},
   "source": [
    "Conveniently enough, every second of a YouTube video is individually linkable\n",
    "and the transcripts come with timestamps.\n",
    "\n",
    "But a second of speech is not a useful source.\n",
    "\n",
    "And by default, the subtitles come \"chunked\" in time\n",
    "at too fine a grain as well:\n",
    "more like five seconds than the thirty to sixty seconds\n",
    "that it takes to make a reasonable point.\n",
    "\n",
    "So now,\n",
    "we combine the five-second subtitle timestamps\n",
    "into longer chunks based on character count --\n",
    "750 seems to generate nicely sized chunks on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf29a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "import srt\n",
    "\n",
    "\n",
    "TRIGGER_LENGTH = 750  # 30-60 seconds\n",
    "\n",
    "def merge(subtitles, idx):\n",
    "    new_content = combine_content(subtitles)\n",
    "\n",
    "    # preserve start as timedelta\n",
    "    new_start = seconds_float_to_timedelta(subtitles[0][\"start\"])\n",
    "    # merge durations as timedelta\n",
    "    new_duration = seconds_float_to_timedelta(sum(sub[\"duration\"] for sub in subtitles))\n",
    "    \n",
    "    # combine\n",
    "    new_end = new_start + new_duration\n",
    "    \n",
    "    return srt.Subtitle(index=idx, start=new_start, end=new_end, content=new_content)\n",
    "\n",
    "\n",
    "def combine_content(subtitles):\n",
    "    contents = [subtitle[\"text\"].strip() for subtitle in subtitles]\n",
    "    return \" \".join(contents) + \"\\n\\n\"\n",
    "\n",
    "\n",
    "def get_charcount(subtitle):\n",
    "    return len(subtitle[\"text\"])\n",
    "\n",
    "\n",
    "def seconds_float_to_timedelta(x_seconds):\n",
    "    return timedelta(seconds=x_seconds)\n",
    "\n",
    "\n",
    "def merge_subtitles(subtitles):\n",
    "    merged_subtitles = []\n",
    "    current_chunk, current_length, chunk_idx = [], 0, 1\n",
    "\n",
    "    for subtitle in subtitles:\n",
    "        current_chunk.append(subtitle)\n",
    "        added_length = get_charcount(subtitle)\n",
    "        new_length = current_length + added_length\n",
    "\n",
    "        if new_length >= TRIGGER_LENGTH:\n",
    "            merged_subtitle = merge(current_chunk, chunk_idx)\n",
    "            merged_subtitles.append(merged_subtitle)\n",
    "            current_chunk, current_length = [], 0\n",
    "            chunk_idx += 1\n",
    "        else:\n",
    "            current_length = new_length\n",
    "\n",
    "    if current_chunk:\n",
    "        merged_subtitle = merge(current_chunk, chunk_idx)\n",
    "        merged_subtitles.append(merged_subtitle)\n",
    "\n",
    "    return merged_subtitles\n",
    "\n",
    "\n",
    "subtitle_collections = [merge_subtitles(transcript) for transcript in transcripts]\n",
    "\n",
    "# get strings as well for quick checks (and easier to write to files)\n",
    "subtitle_strings = [srt.compose(merged_subtitles) for merged_subtitles in subtitle_collections]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a837a221",
   "metadata": {},
   "source": [
    "We then add YouTube URLs\n",
    "for those longer subtitles as sources\n",
    "and combine them into a single `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac34499",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_format = \"https://www.youtube.com/watch?v={id}\"\n",
    "query_params_format = \"&t={start}s\"\n",
    "\n",
    "\n",
    "def create_split_video_df(subtitles, base_url):\n",
    "    rows = []\n",
    "    for subtitle in subtitles:\n",
    "        raw_text = subtitle.content\n",
    "        text = raw_text.strip()\n",
    "        start = timestamp_from_timedelta(subtitle.start)\n",
    "        url = base_url + query_params_format.format(start=start)\n",
    "\n",
    "        rows.append({\"text\": text, \"source\": url})\n",
    "\n",
    "    video_df = pd.DataFrame.from_records(rows)\n",
    "    return video_df\n",
    "\n",
    "\n",
    "def timestamp_from_timedelta(td):\n",
    "    return int(td.total_seconds())\n",
    "\n",
    "\n",
    "split_video_dfs = [\n",
    "    create_split_video_df(subtitles, base_url_format.format(id=video_id))\n",
    "    for subtitles, video_id in zip(subtitle_collections, videos_df.index)\n",
    "]\n",
    "\n",
    "split_video_df = pd.concat(split_video_dfs, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1f9ba06",
   "metadata": {},
   "source": [
    "# Papers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c1a2f6e",
   "metadata": {},
   "source": [
    "Now, let's add some relevant materials from the literature.\n",
    "\n",
    "The [LLM Lit Review](https://tfs.ai/llm-lit-review)\n",
    "is a collection of papers about large language models.\n",
    "\n",
    "Metadata for PDFs of those papers, including URLs, is stored in the `papers.json` file in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4748fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"papers.json\") as f:\n",
    "    pdf_infos = json.load(f)\n",
    "\n",
    "pdf_infos[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5511789f",
   "metadata": {},
   "source": [
    "We can convert those PDFs to text using the tools in `etl.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfe7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import modal\n",
    "\n",
    "pdf_urls = [pdf[\"url\"] for pdf in pdf_infos]\n",
    "extract_pdf = modal.Function.lookup(\"etl-pdfs\", \"extract_pdf\")\n",
    "pages = extract_pdf.call(pdf_urls[0])\n",
    "pages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2a9b9d7",
   "metadata": {},
   "source": [
    "Converting 1 PDF only takes a few seconds.\n",
    "\n",
    "Converting 200 PDFs to text would take nearly 10 minutes and is embarrassingly parallel.\n",
    "\n",
    "So we run that task in parallel on Modal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list(extract_pdf.map(pdf_urls, return_exceptions=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad34b0be",
   "metadata": {},
   "source": [
    "For now, we're writing this piece to the document DB separately from the rest of the corpus,\n",
    "and we also do this on Modal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3714f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_document_db = modal.Function.lookup(\"etl-pdfs\", \"add_to_document_db\")\n",
    "\n",
    "add_to_document_db.call(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efb5307c",
   "metadata": {},
   "source": [
    "# Combine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0dc7025",
   "metadata": {},
   "source": [
    "Now that we've got all of our texts and sources collated\n",
    "in separate `DataFrame`s,\n",
    "let's combine them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [split_lecture_df, split_video_df]\n",
    "document_formatted_dfs = [df[[\"text\", \"source\"]] for df in dfs]\n",
    "document_df = pd.concat(document_formatted_dfs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3de0f43c",
   "metadata": {},
   "source": [
    "Now's a convenient time to add those `sha256` hashes for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "doc_ids = []\n",
    "for _, row in document_df.iterrows():\n",
    "    m = hashlib.sha256()\n",
    "    m.update(row[\"text\"].encode(\"utf-8\"))\n",
    "    doc_ids.append(m.hexdigest())\n",
    "    \n",
    "document_df.index = doc_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54b62e39",
   "metadata": {},
   "source": [
    "Let's look and see how many \"documents\" we ended up with,\n",
    "before we move on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85cc06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(document_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7387789e",
   "metadata": {},
   "source": [
    "## Persist to Disk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4efc2f61",
   "metadata": {},
   "source": [
    "As a first step to persisting our corpus,\n",
    "let's save it to disk and reload it.\n",
    "\n",
    "The data involved is relatively simple --\n",
    "basically all strings --\n",
    "so we don't need to `pickle` the `DataFrame`,\n",
    "which comes with its own woes.\n",
    "\n",
    "Instead, we just format it as `JSON` --\n",
    "the web's favorite serialization format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5513f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_json = document_df.to_json(orient=\"index\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed747fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents.json\", \"w\") as f:\n",
    "    f.write(documents_json)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38a2eb6a",
   "metadata": {},
   "source": [
    "Before moving on,\n",
    "let's check that we can in fact reload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8bf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"documents.json\") as f:\n",
    "    s = f.read()\n",
    "    \n",
    "key, document = list(json.loads(s).items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2998b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(document[\"text\"], document[\"source\"], sep=\"\\n\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760de973",
   "metadata": {},
   "source": [
    "## Put into MongoDB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac4939c2",
   "metadata": {},
   "source": [
    "But a local filesystem isn't a good method for persistence.\n",
    "\n",
    "We want these documents to be available via an API,\n",
    "with the ability to scale reads and writes if needed.\n",
    "\n",
    "So let's put them in a database.\n",
    "\n",
    "We choose MongoDB simply for convenience --\n",
    "we don't want to define a schema just yet,\n",
    "since these tools are evolving rapidly,\n",
    "and there are nice free hosting options.\n",
    "\n",
    "> MongoDB is, in NoSQL terms, a \"document database\",\n",
    "but the term document means something different\n",
    "than it does in \"Document Q&A\".\n",
    "In Mongoland, a \"document\" is just a blob of JSON.\n",
    "We format our Q&A documents as JSON\n",
    "and store them in Mongo,\n",
    "so the distinction is not obvious here."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20e6dd65",
   "metadata": {},
   "source": [
    "If you're running this yourself,\n",
    "you'll need to create a hosted MongoDB instance\n",
    "and add a database called `fsdl`\n",
    "with a collection called `ask-fsdl`.\n",
    "\n",
    "You can find instructions\n",
    "[here](https://www.mongodb.com/basics/mongodb-atlas-tutorial).\n",
    "\n",
    "You'll need the URL and password info\n",
    "from that setup process to connect.\n",
    "\n",
    "Add them to the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b989c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pymongo\n",
    "from pymongo import InsertOne\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "mongodb_url = os.environ[\"MONGODB_URI\"]\n",
    "mongodb_password = os.environ[\"MONGODB_PASSWORD\"]\n",
    "\n",
    "CONNECTION_STRING = f\"mongodb+srv://fsdl:{mongodb_password}@{mongodb_url}/?retryWrites=true&w=majority\"\n",
    "\n",
    "# connect to the database server\n",
    "client = pymongo.MongoClient(CONNECTION_STRING)\n",
    "# connect to the database\n",
    "db = client.get_database(\"fsdl\")\n",
    "# get a representation of the collection\n",
    "collection = db.get_collection(\"ask-fsdl\")\n",
    "\n",
    "collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91e779e",
   "metadata": {},
   "source": [
    "Now that we're connected,\n",
    "we're ready to upsert.\n",
    "\n",
    "We loop over the documents -- loaded from disk --\n",
    "and format them into a Python dictionary\n",
    "that fits our `Document` pseudoschema.\n",
    "\n",
    "With `pymongo`,\n",
    "we can just insert that dictionary directly,\n",
    "using `InsertOne`,\n",
    "and use `bulk_write` to get batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 250\n",
    "requesting = []\n",
    "\n",
    "with open(\"documents.json\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "\n",
    "for (sha_hash, content) in documents.items():\n",
    "    metadata = {key: value for key, value in content.items() if key != \"text\"}\n",
    "    metadata[\"sha256\"] = sha_hash\n",
    "    metadata[\"ignore\"] = False\n",
    "    document = {\"text\": content[\"text\"], \"metadata\": metadata}\n",
    "    requesting.append(InsertOne(document))\n",
    "    \n",
    "    if len(requesting) >= CHUNK_SIZE:\n",
    "        collection.bulk_write(requesting)\n",
    "        requesting = []\n",
    "        \n",
    "if requesting:\n",
    "    collection.bulk_write(requesting)\n",
    "    requesting = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
