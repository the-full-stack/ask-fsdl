{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ETL to Build the Document Corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through the process for setting up the corpus of Full Stack documents that the bot searches over.\n",
    "\n",
    "In each case, we have to\n",
    "- Extract data from its natural habitat, like YouTube or GitHub\n",
    "- Transform it into a format that is useful for our purposes\n",
    "- Load it into our database in that format\n",
    "\n",
    "hence the acronym \"ETL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make secrets  # you'll need credentials for Mongo and Modal to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "\n",
    "from etl import markdown, pdfs, shared, videos\n",
    "from etl.shared import display_modal_image\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDFs: arXiV Papers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!modal run etl/pdfs.py --json-path data/llm-papers.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(shared.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(pdfs.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_path = Path(\"data\") / \"llm-papers.json\"\n",
    "\n",
    "with open(papers_path) as f:\n",
    "    pdf_infos = json.load(f)\n",
    "\n",
    "pdf_urls = [pdf[\"url\"] for pdf in pdf_infos]\n",
    "\n",
    "with pdfs.stub.run():\n",
    "    documents = shared.unchunk(  # each pdf creates multiple documents, so we flatten\n",
    "        # after we run the extract_pdf function on Modal\n",
    "        pdfs.extract_pdf.map(pdf_urls, return_exceptions=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[0][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=documents[0][\"metadata\"][\"source\"], width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    # we split our document list into 10 pieces, so that we don't open too many connections\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(shared.add_to_document_db.map(chunked_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "   # pull only arxiv papers\n",
    "  query = { \"metadata.source\": { \"$regex\": \"arxiv\\.org\", \"$options\": \"i\" } }\n",
    "  # project out the text field, it can get large\n",
    "  projection = {\"text\": 0}\n",
    "  # get just one result to show it worked\n",
    "  result = shared.query_one_document_db.call(query, projection)\n",
    "\n",
    "pp.pprint(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markdown Files: Lectures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!modal run etl/markdown.py --json-path data/lectures-2022.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(markdown.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_path = Path(\"data\") / \"lectures-2022.json\"\n",
    "\n",
    "with open(markdown_path) as f:\n",
    "  markdown_corpus = json.load(f)\n",
    "\n",
    "website_url, md_url = (\n",
    "  markdown_corpus[\"website_url_base\"],\n",
    "  markdown_corpus[\"md_url_base\"],\n",
    ")\n",
    "\n",
    "lectures = markdown_corpus[\"lectures\"]\n",
    "\n",
    "lectures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with markdown.stub.run():\n",
    "    documents = (\n",
    "        shared.unchunk(  # each lecture creates multiple documents, so we flatten\n",
    "            markdown.to_documents.map(\n",
    "                lectures,\n",
    "                kwargs={\"website_url\": website_url, \"md_url\": md_url},\n",
    "                return_exceptions=True,\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[1][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame(src=documents[1][\"metadata\"][\"source\"], width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(shared.add_to_document_db.map(chunked_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "  # pull only lectures\n",
    "  query = { \"metadata.source\": { \"$regex\": \"lecture\", \"$options\": \"i\" } }\n",
    "  # project out the text field, it can get large\n",
    "  projection = {\"text\": 0}\n",
    "  # get just one result to show it worked\n",
    "  result = shared.query_one_document_db.call(query, projection, collection=\"ask-fsdl\")\n",
    "\n",
    "pp.pprint(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Videos: YouTube Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_modal_image(videos.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_path = Path(\"data\") / \"videos.json\"\n",
    "\n",
    "with open(videos_path) as f:\n",
    "    video_infos = json.load(f)\n",
    "\n",
    "video_ids = [video[\"id\"] for video in video_infos]\n",
    "\n",
    "video_infos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with videos.stub.run():\n",
    "    documents = (\n",
    "        shared.unchunk(  # each lecture creates multiple documents, so we flatten\n",
    "            videos.extract_subtitles.map(\n",
    "                video_ids,\n",
    "                return_exceptions=True,\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(documents[1][\"metadata\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "id_str, time_str = documents[1][\"metadata\"][\"source\"].split(\"?v=\")[1].split(\"&t=\")\n",
    "YouTubeVideo(id_str, start=int(time_str.strip(\"s\")), width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "    chunked_documents = shared.chunk_into(documents, 10)\n",
    "    list(shared.add_to_document_db.map(chunked_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with shared.stub.run():\n",
    "  # pull only lectures\n",
    "  query = { \"metadata.source\": { \"$regex\": \"youtube\", \"$options\": \"i\" } }\n",
    "  # project out the text field, it can get large\n",
    "  projection = {\"text\": 0}\n",
    "  # get just one result to show it worked\n",
    "  result = shared.query_one_document_db.call(query, projection, collection=\"ask-fsdl\")\n",
    "\n",
    "pp.pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ask-fsdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
